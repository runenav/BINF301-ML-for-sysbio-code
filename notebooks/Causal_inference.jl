### A Pluto.jl notebook ###
# v0.19.36

using Markdown
using InteractiveUtils

# ╔═╡ 947d626d-f39b-4100-8a3e-fcf087caa355
using DrWatson

# ╔═╡ 7d74bb79-12ad-4329-a0b0-7d0bfe0fd812
# ╠═╡ show_logs = false
quickactivate(@__DIR__)

# ╔═╡ 50ea2369-6afb-43a2-9bc4-64157bc6cf76
begin
	using Random
	using Distributions
	using StatsBase
	using DataFrames
	using GLM
	using LinearAlgebra
	#using CairoMakie
	using LaTeXStrings
end

# ╔═╡ cd2140c0-ca5a-11ee-2721-ef7db51463b0
md"# Causal inference and model selection

## Setup the environment

Activate the BINF301 environment and load packages:
"

# ╔═╡ 572dd7e7-598a-4127-9d51-a1879c686bed
md"""
## Background

In this tutorial we consider two continuous and correlated variables, ``X`` and ``Y``, representing the expression levels of two genes. We also consider a discrete variable ``Z`` representing a genotype for gene ``X``. Typically, ``Z`` will have been obtained by [eQTL mapping](https://en.wikipedia.org/wiki/Expression_quantitative_trait_loci) for gene ``X``. We wish to determine if variation in ``X`` causes variation in ``Y``.

The **aim of causal model selection** is: given observational data for ``Z``, ``X``, and ``Y`` in a set of independent samples, which causal model (represented by a directed acyclic graph) explains the data best. 

To restrict the space of possible models that need to be considered, a number of assumptions reflecting biological knowledge can be made:

1. Genetic variation influences variation in gene expression (and phenotypes more generally), but changing the value of an individual's phenotype does not change their genome. Hence, in our models there can be **no incoming arrows into ``Z``**.

2. We assume that the statistical association between ``Z`` and ``X`` is due to a *direct* effect, that is, all causal models we consider **must contain the directed edge ``Z\to X``**. This assumption is justified if ``Z`` is located within or near to ``X`` (on the genome) and in a known regulatory region for ``X``.

3. For ``X`` and ``Y`` to be correlated (non-independent), there must be a path in the graph between them, more precisely  **``X`` and ``Y`` must not be** [**d-separated**](http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html).

4. By the laws of [Mendelian inheritance](https://en.wikipedia.org/wiki/Mendelian_inheritance) (particularly the random segregation of alleles), we may assume that ``Z`` is independent of any unobserved confounding factors ``U`` that cause ``X`` and ``Y`` to be correlated, and therefore **there are no edges between ``Z`` and any unobserved ``U``**.

If we assume that there are **no unobserved factors**, there are 5 possible models satisfying assumptions 1-4 (see figure below). If we allow for the **presence of unobserved factors**, we have the same 5 models with an additional unobserved ``U`` having a causal effect on ``X`` and ``Y``, plus one more model without an edge between ``X`` and ``Y`` where all of their correlation is explained by ``U`` (see figure).


Below each model in the figure, some of the key conditional independences implied by the model are shown, using the mathematical notation ``⫫`` for "is independent of", ``∣`` for "conditional on", ``∧`` for "and", and ``¬`` for "it is not the case that".

From this figure it is immediately clear that our aim of deciding whether ``X`` causes ``Y`` is unachievable. Even without any unobserved factors, models 4a and 5a are [Markov equivalent](https://www.cs.ru.nl/~peterl/markoveq.pdf): they entail the same conditional independences and are indistinguishable using observational data. In other words, **there exists no condition that is both necessary and sufficient** for ``X`` to cause ``Y`` given the above 4 assumptions.

There are two possible paths forward (apart from giving up): to **use a sufficient condition  or a necessary condition** for causality. If the joint probability distribution of ``Z``, ``X``, and ``Y`` passes a sufficient condition, then it is guaranteed to have been generated by a model where ``X`` causes ``Y``, but there may be distributions with ``X\to Y`` that don't pass the test (false negatives). Conversely, all joint distributions generated by models with ``X\to Y`` will pass a necessary condition, but there may be distributions generated by models where ``X`` does not cause ``Y`` that also pass the test (false positives).

Because estimating the joint distribution of ``Z``, ``X``, and ``Y`` and its conditional independences from a finite number  of samples is itself an imperfect process that can only ever approximate the true distribution, having additional errors from using an imperfect causality test is not necessarily catastrophic, provided those errors are small enough. 

## A sufficient condition for ``X\to Y``

Our sufficient condition for ``X\to Y`` is based on model 1a. This model implies that ``Z`` and ``Y`` are not independent, but ``Z`` is independent of ``Y`` conditioned on ``X`` (``X`` is a **mediator** for the causal path from ``Z`` to ``Y``). No other model satisfies those two relations:

- In models 2a, 2b, and 6b, ``Z`` and ``Y`` are independent.
- In all other models ``Z`` is not independent of ``Y`` conditioned on ``X``, either because there is a direct path from ``Z`` to ``Y`` not passing through ``X``, or because conditioning on ``X`` opens a path from ``Z`` to ``Y`` via the confounder ``U`` (due to the v-structure or [collider](https://en.wikipedia.org/wiki/Collider_(statistics)) at ``X``).

Hence any joint distribution in which ``Z`` and ``Y`` are not independent, but ``Z`` is independent of ``Y`` conditioned on ``X``, must have been generated by model 1a, that is, by a model where ``X\to Y``. In mathematical notation:

```math
¬(Z⫫Y) ∧ (Z⫫Y∣X) ⇒ (X→Y)
```

## A necessary condition for ``X\to Y``

Because all models contain the edge ``G→X``, it follows that if ``X→Y``, then ``Z`` and ``Y`` cannot be independent, providing a simple necessary condition for ``X→Y``. However, ``Z`` and ``Y`` are also not independent in models 3a-b and 5a-b, in which ``Y→X``, because of the direct edge ``G→Y``. Of these, only 3a can be excluded, because in 3a, ``X⫫Y∣Z``, a condition not satisfied in any model where ``X→Y``. Combining these two results, we obtain

```math
(X→Y) ⇒ ¬(Z⫫Y) ∧ ¬(X⫫Y∣Z)
```
"""

# ╔═╡ 29e76b70-0641-4b03-aaba-83171828b665
md"""
## Data simulation

First we set a random seed and fix the number of samples:
"""

# ╔═╡ 49e79fb6-2371-4da5-8819-07956bdf52e8
Random.seed!(123);

# ╔═╡ 5775d34b-409c-4e35-a678-bde1a0d89bde
N = 200;

# ╔═╡ 27e33a19-8bd8-482c-825f-b08bf121b94e
md"""
In our simulations, as in real biology, a genotype value is sampled first. Then expression values for the two genes are sampled conditional on the genotype value.

### Genotype data simulation

We simulate the genotype of a bi-allelic (2 values), diploid (2 copies) polymorphism. We encode the major and minor alleles by the values 0 and 1, respectively. The genotype is sampled by defining the minor allele frequency (MAF) as a parameter of the simulation. Two haploids are sampled using a Bernoulli distribution and summed to give the genotype, that is, the genotype is the number of copies of the minor allele in an individual. 

Because the mean of a Bernoulli distribution with probability of success ``p`` is ``p``, the mean of ``Z`` is 2 times the minor allele frequency, and we therefore subtract this value from the sampled genotypes to obtain samples from a random variable ``Z`` with mean zero. Note that we *cannot* center the actual sampled values at this point, because we still need to generate the expression data conditional on ``Z``, and the expression levels of an individual cannot depend on the sample mean of the genotypes in a population! 
"""

# ╔═╡ ae6ecc6c-6cbc-4da6-bacd-c0fa60845dd2
begin
	maf = 0.3
	H1 = rand(Bernoulli(maf),N)
	H2 = rand(Bernoulli(maf),N)
	Z0 = H1 .+ H2 
	Z = Z0 .- 2*maf;
end;

# ╔═╡ 590eece9-3e35-4cf7-9d2d-059d16dad0c4
md"""
### Expression data simulation

To simulate data for ``X`` and ``Y``, we must first set up the [structural equations](https://en.wikipedia.org/wiki/Structural_equation_modeling) for the [causal model](https://en.wikipedia.org/wiki/Causal_model) we want to simulate. We will assume linear models with additive Gaussian noise throughout. 

Let's start by models 1a, 1b, and 6b. Their structural equations can be written as 
"""

# ╔═╡ Cell order:
# ╟─cd2140c0-ca5a-11ee-2721-ef7db51463b0
# ╠═947d626d-f39b-4100-8a3e-fcf087caa355
# ╠═7d74bb79-12ad-4329-a0b0-7d0bfe0fd812
# ╠═50ea2369-6afb-43a2-9bc4-64157bc6cf76
# ╟─572dd7e7-598a-4127-9d51-a1879c686bed
# ╟─29e76b70-0641-4b03-aaba-83171828b665
# ╠═49e79fb6-2371-4da5-8819-07956bdf52e8
# ╠═5775d34b-409c-4e35-a678-bde1a0d89bde
# ╟─27e33a19-8bd8-482c-825f-b08bf121b94e
# ╠═ae6ecc6c-6cbc-4da6-bacd-c0fa60845dd2
# ╟─590eece9-3e35-4cf7-9d2d-059d16dad0c4
